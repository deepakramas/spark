{
 "metadata": {
  "name": "",
  "signature": "sha256:86b9f67828c971787c918654f5a46413172c51c4018e25ffc7eb1cf4695f4e70"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "PySpark has a built-in Spark context called 'sc'. Or could invoke your own. In this single client session, only one context is available/creatable."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print sc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<pyspark.context.SparkContext object at 0x273c550>\n"
       ]
      }
     ],
     "prompt_number": 43
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "Read in a small dataset, movie ratings, and persist/cache it so it remains after the session. The ratings are of the format (userid, movieid, rating). The dataset is in hdfs but it could also be hbase, s3 etc."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = sc.textFile('smallmovietest.data')\n",
      "data.cache()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 93,
       "text": [
        "MappedRDD[104] at textFile at NativeMethodAccessorImpl.java:-2"
       ]
      }
     ],
     "prompt_number": 93
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "'data' looks this ..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data.take(5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 160,
       "text": [
        "[u'1662,1427,3', u'5007,127,8', u'5638,980,7', u'2375,2964,3', u'785,3793,8']"
       ]
      }
     ],
     "prompt_number": 160
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "'data' is an RDD (Resilient Distributed Dataset) representing a Hadoop dataset on which one can perform computations. Below the data is split into tuples and represented as another RDD, 'myRdd'. Spark uses lazy evaluation to optimize the DAG."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_tuple(x) :\n",
      "    y = x.split(',')\n",
      "    return int(y[0]), y[1], int(y[2])\n",
      "myRdd = data.flatMap(lambda x: x.split(' ')).map(get_tuple)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 102
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "print the first 5 records in the client and unpersist 'data' because we don't need it anymore"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data.unpersist()\n",
      "myRdd.take(5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 103,
       "text": [
        "[(1662, u'1427', 3),\n",
        " (5007, u'127', 8),\n",
        " (5638, u'980', 7),\n",
        " (2375, u'2964', 3),\n",
        " (785, u'3793', 8)]"
       ]
      }
     ],
     "prompt_number": 103
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "group by key=rating and then map and reduce by key i.e. count the number of reviews per rating. Notice how the Map and Reduce steps are combined into a single 'reduceByKey'."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rating_counts = myRdd.map(lambda x: (x[2],1))\n",
      "from operator import add\n",
      "myAggregateRdd = rating_counts.reduceByKey(add)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 126
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "look at the results per review. 'collect' returns the aggregated ratings as a python list."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "myAggregateRdd.collect()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 127,
       "text": [
        "[(0, 10114),\n",
        " (1, 9860),\n",
        " (2, 10055),\n",
        " (3, 10024),\n",
        " (4, 10070),\n",
        " (5, 9810),\n",
        " (6, 10038),\n",
        " (7, 10035),\n",
        " (8, 10044),\n",
        " (9, 9950)]"
       ]
      }
     ],
     "prompt_number": 127
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "compute average rating. Do it two ways. Using 'reduce' and then via an accumulator. Of course there are other ways as well. accumulator are like 'global' variables that are shipped to each partition and can only be added into."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "myRdd.map(lambda x: x[2]).reduce(add)*1.0/myRdd.count()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 129,
       "text": [
        "4.49747"
       ]
      }
     ],
     "prompt_number": 129
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "a = sc.accumulator(0)\n",
      "def adder(x):\n",
      "    global a\n",
      "    a += x[1]*x[0]\n",
      "myAggregateRdd.foreach(adder)\n",
      "print a.value*1.0/myRdd.count()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "4.49747\n"
       ]
      }
     ],
     "prompt_number": 158
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}